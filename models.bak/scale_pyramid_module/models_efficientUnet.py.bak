import torch.nn as nn
import torch.utils.model_zoo as model_zoo
from torch.nn import functional as F

# ADD
import math
from batchnorm import SynchronizedBatchNorm2d
from contextual_layer import ContextualModule
import torch
from collections import OrderedDict
from layers import *
from efficientnet import EfficientNet


#__all__ = ['EfficientUnet', 'get_efficientunet_b0', 'get_efficientunet_b1', 'get_efficientunet_b2',
#           'get_efficientunet_b3', 'get_efficientunet_b4', 'get_efficientunet_b5', 'get_efficientunet_b6',
#           'get_efficientunet_b7']



__all__ = ['vgg19', 'vgg16']
model_urls = {
    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',
}

#class VGG(nn.Module):
#    def __init__(self, features):
#        super(VGG, self).__init__()
#        self.features = features # vgg19 bottleneck
#        self.reg_layer = nn.Sequential(
#            nn.Conv2d(512, 256, kernel_size=3, padding=1),
#            nn.ReLU(inplace=True),
#            nn.Conv2d(256, 128, kernel_size=3, padding=1),
#            nn.ReLU(inplace=True),
#        )
#        self.density_layer = nn.Sequential(nn.Conv2d(128, 1, 1), nn.ReLU())
#
#    def forward(self, x):
#        x = self.features(x)
#        x = F.upsample_bilinear(x, scale_factor=2)
#        x = self.reg_layer(x)
#        mu = self.density_layer(x)
#        B, C, H, W = mu.size()
#        mu_sum = mu.view([B, -1]).sum(1).unsqueeze(1).unsqueeze(2).unsqueeze(3)
#        mu_normed = mu / (mu_sum + 1e-6)
#        return mu, mu_normed
#
#def make_layers(cfg, batch_norm=False):
#    layers = []
#    in_channels = 3
#    for v in cfg:
#        if v == 'M':
#            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
#        else:
#            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
#            if batch_norm:
#                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]
#            else:
#                layers += [conv2d, nn.ReLU(inplace=True)]
#            in_channels = v
#    return nn.Sequential(*layers)
#
#cfg = {
#    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]
#}
#
#def vgg19():
#    """VGG 19-layer model (configuration "E")
#        model pre-trained on ImageNet
#    """
#    model = VGG(make_layers(cfg['E']))
#    model.load_state_dict(model_zoo.load_url(model_urls['vgg19']), strict=False)
#    return model


def vgg19():
    return None


def vgg16():
    """VGG 16-layer model based M-SFANet
    """
    return Model()


class _ASPPModule(nn.Module):
    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):
        super(_ASPPModule, self).__init__()
        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,
                                            stride=1, padding=padding, dilation=dilation, bias=False)
        self.bn = BatchNorm(planes)
        self.relu = nn.ReLU()

        self._init_weight()

    def forward(self, x):
        x = self.atrous_conv(x)
        x = self.bn(x)

        return self.relu(x)

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, SynchronizedBatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

class ASPP(nn.Module):
    def __init__(self, backbone, output_stride, BatchNorm):
        super(ASPP, self).__init__()
        if backbone == 'drn':
            inplanes = 512
        elif backbone == 'mobilenet':
            inplanes = 320
        elif backbone == 'efficientnet':
            #inplanes = 144 # B6
            inplanes = 128 # B5
        else:
            inplanes = 512
        if output_stride == 16:
            dilations = [1, 6, 12, 18]
        elif output_stride == 8:
            dilations = [1, 12, 24, 36]
        else:
            raise NotImplementedError

        self.aspp1 = _ASPPModule(inplanes, inplanes, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)
        self.aspp2 = _ASPPModule(inplanes, inplanes, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)
        self.aspp3 = _ASPPModule(inplanes, inplanes, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)
        self.aspp4 = _ASPPModule(inplanes, inplanes, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)

        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),
        #self.global_avg_pool = nn.Sequential(nn.AvgPool2d(32,32), # for 512*512
        #self.global_avg_pool = nn.Sequential(nn.AvgPool2d(16,16), # for 256*256
                                             nn.Conv2d(inplanes, inplanes, 1, stride=1, bias=False),
#                                             BatchNorm(256),
                                             nn.ReLU())
        self.conv1 = nn.Conv2d(inplanes*5, inplanes, 1, bias=False)
        self.bn1 = BatchNorm(inplanes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self._init_weight()

    def forward(self, x):
        x1 = self.aspp1(x)
        x2 = self.aspp2(x)
        x3 = self.aspp3(x)
        x4 = self.aspp4(x)
        x5 = self.global_avg_pool(x)
        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)
        x = torch.cat((x1, x2, x3, x4, x5), dim=1)

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        return self.dropout(x)

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, SynchronizedBatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
                
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        #from efficientunet import get_efficientunet_b5
        self.b5unet = get_efficientunet_b5(out_channels=1, concat_input=True, pretrained=True)
#        self.vgg = VGG()
#        self.load_vgg()
#        self.spm = ScalePyramidModule()
#        self.amp = BackEnd()
#        self.dmp = BackEnd()

        #self.conv_att = BaseConv(32, 1, 1, 1, activation=nn.Sigmoid(), use_bn=True)
        #self.conv_att = BaseConv(256, 1, 1, 1, activation=nn.Sigmoid(), use_bn=True)
        #self.conv_out = BaseConv(32, 1, 1, 1, activation=None, use_bn=False)

        # ADD branchs
#        self.branch_1 = nn.Sequential(nn.Linear(512, 64),
#                                      nn.ReLU(inplace=True),
#                                      nn.Dropout(p=0.5)) # Linear(512, 64) down4のoutput512次元ベクトル->64次元
        #self.branch_2 = nn.Sequential(nn.Linear((256/2)*(256/2), 64),
#        self.branch_2 = nn.Sequential(nn.Linear((256)*(256), 64),
#                                      nn.ReLU(inplace=True),
#                                      nn.Dropout(p=0.5)) # Linear(128*128, 64) up8のoutput128*128次元ベクトル->64次元(down*4, up*3なのでinput(256*256)の1/2サイズになっている)
        #self.outc = outconv(32, out_ch=1) # 1ch liklihood map [0,1]
        #self.outc = outconv(256, out_ch=1) # 1ch liklihood map [0,1]
        #self.out_nonlin = nn.Sigmoid()
#        self.regressor = nn.Sequential(nn.Linear(64 + 64, 1),
#                                       nn.ReLU()) # branch1 + branch2 = 64 + 64 -> output 1 scaler

#        self.down5 = down(512, 512)
#        self.down6 = down(512, 512)
#        self.down7 = down(512, 512)
#        self.down8 = down(512, 512, normaliz=False)

#        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),
#                                             nn.Conv2d(512, 512, 1, stride=1, bias=False), # point-wise
#                                             nn.BatchNorm2d(512),
#                                             nn.ReLU())


#        self.conv_att = nn.Sequential(
#            nn.Conv2d(32, 32, kernel_size=3, padding=1),
#            nn.ReLU(inplace=True),
#            nn.Conv2d(32, 32, kernel_size=3, padding=1),
#            nn.ReLU(inplace=True),
#            nn.Conv2d(32, 1, kernel_size=1),
#            nn.Sigmoid(),
#        )
# 
        self.density_layer = nn.Sequential(
            nn.Conv2d(1, 1, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
#            nn.Conv2d(32, 32, kernel_size=3, padding=1),
#            nn.ReLU(inplace=True),
#            nn.Conv2d(32, 1, kernel_size=1),
#            nn.ReLU(), # not use activation!
        )
 

    def forward(self, input):
#        input = self.vgg(input)
        input = self.b5unet(input)
        x = input
        #x = self.density_layer(x)

#        # ADD branch_1
#        _conv1_2, _conv2_2, _conv3_3, _conv4_3, conv5_3 = input
#        x = self.global_avg_pool(conv5_3)
#        middle_layer = x
#        middle_layer_flat = middle_layer.view(middle_layer.shape[0], -1) # flat化(バッチサイズの行)
#        lateral_flat = self.branch_1(middle_layer_flat) # input flattened output of down8 layer

#        spm_out = self.spm(*input) # input to ASPP & CAN module. return modified (conv2_2, conv3_3, conv4_3, conv5_3)
#        amp_out = self.amp(*spm_out)
#        dmp_out = self.dmp(*spm_out) # output 32-ch map
#
#        # ADD branch_2
#        amp_out = self.conv_att(amp_out)
#        dmp_out = amp_out * dmp_out
#        #x = self.outc(dmp_out) # 32-ch map -> 1-ch activation map
#        #x = self.out_nonlin(x) # 1-ch probability map [0,1]
#        x = self.density_layer(dmp_out)
#
        # Reshape Bx1xHxW -> BxHxW
        # because probability map is real-valued by definition
#        x = x.squeeze(1)

#        x_flat = x.view(x.shape[0], -1) # flat化(バッチサイズの行)
#        x_flat = self.branch_2(x_flat) # input flattened output of up8 layer
#        regression_features = torch.cat((x_flat, lateral_flat), dim=1) # concat
#        regression = self.regressor(regression_features) # ..and regress
#
#        return x, regression

        mu = x
        B, C, H, W = mu.size()
        mu_sum = mu.view([B, -1]).sum(1).unsqueeze(1).unsqueeze(2).unsqueeze(3)
        mu_normed = mu / (mu_sum + 1e-6)
        return mu, mu_normed


    def load_vgg(self):
        state_dict = model_zoo.load_url('https://download.pytorch.org/models/vgg16_bn-6c64b313.pth')
        old_name = [0, 1, 3, 4, 7, 8, 10, 11, 14, 15, 17, 18, 20, 21, 24, 25, 27, 28, 30, 31, 34, 35, 37, 38, 40, 41]
        new_name = ['1_1', '1_2', '2_1', '2_2', '3_1', '3_2', '3_3', '4_1', '4_2', '4_3', '5_1', '5_2', '5_3']
        new_dict = {}
        for i in range(13):
            new_dict['conv' + new_name[i] + '.conv.weight'] = \
                state_dict['features.' + str(old_name[2 * i]) + '.weight']
            new_dict['conv' + new_name[i] + '.conv.bias'] = \
                state_dict['features.' + str(old_name[2 * i]) + '.bias']
            new_dict['conv' + new_name[i] + '.bn.weight'] = \
                state_dict['features.' + str(old_name[2 * i + 1]) + '.weight']
            new_dict['conv' + new_name[i] + '.bn.bias'] = \
                state_dict['features.' + str(old_name[2 * i + 1]) + '.bias']
            new_dict['conv' + new_name[i] + '.bn.running_mean'] = \
                state_dict['features.' + str(old_name[2 * i + 1]) + '.running_mean']
            new_dict['conv' + new_name[i] + '.bn.running_var'] = \
                state_dict['features.' + str(old_name[2 * i + 1]) + '.running_var']

        self.vgg.load_state_dict(new_dict)


class VGG(nn.Module):
    def __init__(self):
        super(VGG, self).__init__()
        self.pool = nn.MaxPool2d(2, 2)
        self.conv1_1 = BaseConv(3, 64, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv1_2 = BaseConv(64, 64, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv2_1 = BaseConv(64, 128, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv2_2 = BaseConv(128, 128, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv3_1 = BaseConv(128, 256, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv3_2 = BaseConv(256, 256, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv3_3 = BaseConv(256, 256, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv4_1 = BaseConv(256, 512, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv4_2 = BaseConv(512, 512, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv4_3 = BaseConv(512, 512, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv5_1 = BaseConv(512, 512, 3, 1, activation=nn.ReLU(), use_bn=True) # use_bn=False
        self.conv5_2 = BaseConv(512, 512, 3, 1, activation=nn.ReLU(), use_bn=True) # use_bn=False
        self.conv5_3 = BaseConv(512, 512, 3, 1, activation=nn.ReLU(), use_bn=True) # use_bn=False

    def forward(self, input):
        input = self.conv1_1(input)
        conv1_2 = self.conv1_2(input) # add
         
        input = self.pool(conv1_2)
        input = self.conv2_1(input)
        conv2_2 = self.conv2_2(input)

        input = self.pool(conv2_2)
        input = self.conv3_1(input)
        input = self.conv3_2(input)
        conv3_3 = self.conv3_3(input)

        input = self.pool(conv3_3)
        input = self.conv4_1(input)
        input = self.conv4_2(input)
        conv4_3 = self.conv4_3(input)

        input = self.pool(conv4_3)
        input = self.conv5_1(input)
        input = self.conv5_2(input)
        conv5_3 = self.conv5_3(input)

#        return conv2_2, conv3_3, conv4_3, conv5_3
        return conv1_2, conv2_2, conv3_3, conv4_3, conv5_3



class BackEnd(nn.Module):
    def __init__(self):
        super(BackEnd, self).__init__()
        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)
        self.upsample4 = nn.UpsamplingBilinear2d(scale_factor=4)
        
        self.conv01 = BaseConv(2176, 512, 1, 1, activation=nn.ReLU(), use_bn=True) # B5
        #self.conv01 = BaseConv(2448, 512, 1, 1, activation=nn.ReLU(), use_bn=True) # B6
        self.conv02 = BaseConv(512, 512, 1, 1, activation=nn.ReLU(), use_bn=True)

        self.conv1 = BaseConv(576, 256, 1, 1, activation=nn.ReLU(), use_bn=True) # B5
        #self.conv1 = BaseConv(584, 256, 1, 1, activation=nn.ReLU(), use_bn=True) # B6
        self.conv2 = BaseConv(256, 256, 3, 1, activation=nn.ReLU(), use_bn=True)
        #self.conv2_2 = BaseConv(256, 256, 3, 1, activation=nn.ReLU(), use_bn=True)
        
        self.conv3 = BaseConv(424, 128, 1, 1, activation=nn.ReLU(), use_bn=True) # B5
        #self.conv3 = BaseConv(440, 128, 1, 1, activation=nn.ReLU(), use_bn=True) # B6
        self.conv4 = BaseConv(128, 128, 3, 1, activation=nn.ReLU(), use_bn=True)
        #self.conv4_2 = BaseConv(128, 128, 3, 1, activation=nn.ReLU(), use_bn=True)

        self.conv5 = BaseConv(152, 64, 1, 1, activation=nn.ReLU(), use_bn=True) # B5
        #self.conv5 = BaseConv(160, 64, 1, 1, activation=nn.ReLU(), use_bn=True) # B6
        self.conv6 = BaseConv(64, 64, 3, 1, activation=nn.ReLU(), use_bn=True)
        self.conv7 = BaseConv(64, 32, 3, 1, activation=nn.ReLU(), use_bn=True)

        self.conv8 = BaseConv(32, 32, 3, 1, activation=nn.ReLU(), use_bn=True) # activation=None?
        #self.conv8 = BaseConv(96, 32, 3, 1, activation=nn.ReLU(), use_bn=True) # activation=None?
        self.conv9 = BaseConv(32, 32, 3, 1, activation=nn.ReLU(), use_bn=True) # activation=None?

    def forward(self, *input):
        #conv2_2, conv3_3, conv4_3, conv5_3 = input
        conv1_2, conv2_2, conv3_3, conv4_3, conv5_3 = input

        input = self.upsample(conv5_3)
        
        input = torch.cat([input, conv4_3], 1)
        input = self.conv01(input)
        input = self.conv02(input)
 

        #input = self.upsample(conv4_3)
        input = self.upsample(input)
        
        input = torch.cat([input, conv3_3], 1)
        input = self.conv1(input)
        input = self.conv2(input)
        #input = self.conv2_2(input)
        input = self.upsample(input)
        
        input = torch.cat([input, conv2_2, self.upsample4(conv4_3)], 1)
        input = self.conv3(input)
        input = self.conv4(input)
        #input = self.conv4_2(input)
        input = self.upsample(input)

        input = torch.cat([input, conv1_2], 1)
        input = self.conv5(input)
        input = self.conv6(input)
        input = self.conv7(input)
        # ADD
#        input = self.upsample(input)
#        #input = torch.cat([input, conv1_2], 1)
#        input = self.conv8(input)
#        input = self.conv9(input)
 
        return input


class BaseConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel, stride=1, activation=None, use_bn=False):
        super(BaseConv, self).__init__()
        self.use_bn = use_bn
        self.activation = activation
        self.conv = nn.Conv2d(in_channels, out_channels, kernel, stride, kernel // 2)
        self.conv.weight.data.normal_(0, 0.01)
        self.conv.bias.data.zero_()
        self.bn = SynchronizedBatchNorm2d(out_channels)
        self.bn.weight.data.fill_(1)
        self.bn.bias.data.zero_()

    def forward(self, input):
        input = self.conv(input)
        if self.use_bn:
            input = self.bn(input)
        if self.activation:
            input = self.activation(input)

        return input

# ADD
class outconv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(outconv, self).__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 1)
        # self.conv = nn.Sequential(
            # nn.Conv2d(in_ch, out_ch, 1),
        # )

    def forward(self, x):
        x = self.conv(x)
        return x

class down(nn.Module):
    def __init__(self, in_ch, out_ch, normaliz=True):
        super(down, self).__init__()
        self.mpconv = nn.Sequential(
            nn.MaxPool2d(2),
            double_conv(in_ch, out_ch, normaliz=normaliz)
        )

    def forward(self, x):
        x = self.mpconv(x)
        return x

class double_conv(nn.Module):
    def __init__(self, in_ch, out_ch, normaliz=True, activ=True):
        super(double_conv, self).__init__()

        ops = []
        ops += [nn.Conv2d(in_ch, out_ch, 3, padding=1)]
        # ops += [nn.Dropout(p=0.1)]
        if normaliz:
            ops += [nn.BatchNorm2d(out_ch)]
        if activ:
            ops += [nn.ReLU(inplace=True)]
        ops += [nn.Conv2d(out_ch, out_ch, 3, padding=1)]
        # ops += [nn.Dropout(p=0.1)]
        if normaliz:
            ops += [nn.BatchNorm2d(out_ch)]
        if activ:
            ops += [nn.ReLU(inplace=True)]

        self.conv = nn.Sequential(*ops)

    def forward(self, x):
        x = self.conv(x)
        return x




def get_blocks_to_be_concat(model, x):
    shapes = set()
    blocks = OrderedDict()
    hooks = []
    count = 0

    def register_hook(module):

        def hook(module, input, output):
            try:
                nonlocal count
                if module.name == f'blocks_{count}_output_batch_norm':
                    count += 1
                    shape = output.size()[-2:]
                    if shape not in shapes:
                        shapes.add(shape)
                        blocks[module.name] = output

                elif module.name == 'head_swish':
                    # when module.name == 'head_swish', it means the program has already got all necessary blocks for
                    # concatenation. In my dynamic unet implementation, I first upscale the output of the backbone,
                    # (in this case it's the output of 'head_swish') concatenate it with a block which has the same
                    # Height & Width (image size). Therefore, after upscaling, the output of 'head_swish' has bigger
                    # image size. The last block has the same image size as 'head_swish' before upscaling. So we don't
                    # really need the last block for concatenation. That's why I wrote `blocks.popitem()`.
                    blocks.popitem()
                    blocks[module.name] = output

            except AttributeError:
                pass

        if (
                not isinstance(module, nn.Sequential)
                and not isinstance(module, nn.ModuleList)
                and not (module == model)
        ):
            hooks.append(module.register_forward_hook(hook))

    # register hook
    model.apply(register_hook)

    # make a forward pass to trigger the hooks
    model(x)

    # remove these hooks
    for h in hooks:
        h.remove()

    return blocks


class EfficientUnet(nn.Module):
    def __init__(self, encoder, out_channels=2, concat_input=True):
        super().__init__()

        self.encoder = encoder
        self.concat_input = concat_input

        self.up_conv1 = up_conv(self.n_channels, 512)
        self.double_conv1 = double_conv(self.size[0], 512)
        self.up_conv2 = up_conv(512, 256)
        self.double_conv2 = double_conv(self.size[1], 256)
        self.up_conv3 = up_conv(256, 128)
        self.double_conv3 = double_conv(self.size[2], 128)
        self.up_conv4 = up_conv(128, 64)
        self.double_conv4 = double_conv(self.size[3], 64)

        self.up_conv_input = up_conv(32, 32)
        self.double_conv_input = double_conv(32, 32)
        if self.concat_input:
            self.up_conv_input = up_conv(32, 32)
            #self.up_conv_input = up_conv(32, 32)
            self.double_conv_input = double_conv(35, 32)

        self.final_conv = nn.Conv2d(self.size[5], out_channels, kernel_size=1)

        # ADD
        self.spm = ScalePyramidModule()
        self.amp = BackEnd()
        self.dmp = BackEnd()
        self.conv_att = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1),
            nn.Sigmoid(),
        )
        self.density_layer = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 1, kernel_size=1),
            nn.ReLU(), # not use activation!
        )
 



    @property
    def n_channels(self):
        n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,
                           'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,
                           'efficientnet-b6': 2304, 'efficientnet-b7': 2560}
        return n_channels_dict[self.encoder.name]

    @property
    def size(self):
        size_dict = {'efficientnet-b0': [592, 296, 152, 80, 35, 32], 'efficientnet-b1': [592, 296, 152, 80, 35, 32],
                     'efficientnet-b2': [600, 304, 152, 80, 35, 32], 'efficientnet-b3': [608, 304, 160, 88, 35, 32],
                     'efficientnet-b4': [624, 312, 160, 88, 35, 32], 'efficientnet-b5': [640, 320, 168, 88, 35, 32],
                     'efficientnet-b6': [656, 328, 168, 96, 35, 32], 'efficientnet-b7': [672, 336, 176, 96, 35, 32]}
        return size_dict[self.encoder.name]

    def forward(self, x):
        input_ = x

        blocks = get_blocks_to_be_concat(self.encoder, x)
        blocks_val = [v for k,v in blocks.items()]
        #_, x = blocks.popitem()

        # ADD  decoder
        spm_out = self.spm(*blocks_val) # input to ASPP & CAN module. return modified (conv2_2, conv3_3, conv4_3, conv5_3)
        amp_out = self.amp(*spm_out)
        dmp_out = self.dmp(*spm_out) # output 32-ch map
        amp_out = self.up_conv_input(amp_out)
        dmp_out = self.up_conv_input(dmp_out)
        if self.concat_input:
            amp_out = torch.cat([amp_out, input_], dim=1)
            dmp_out = torch.cat([dmp_out, input_], dim=1)
        amp_out = self.double_conv_input(amp_out)
        dmp_out = self.double_conv_input(dmp_out)



        # ADD branch_2
        amp_out = self.conv_att(amp_out)
        dmp_out = amp_out * dmp_out
        #x = self.outc(dmp_out) # 32-ch map -> 1-ch activation map
        #x = self.out_nonlin(x) # 1-ch probability map [0,1]
        x = self.density_layer(dmp_out)

       # Reshape Bx1xHxW -> BxHxW
       # because probability map is real-valued by definition
        #x = x.squeeze(1)
        # TO ADD



        #x = self.up_conv1(x)
        #x = torch.cat([x, blocks.popitem()[1]], dim=1)
        #x = self.double_conv1(x)

        #x = self.up_conv2(x)
        #x = torch.cat([x, blocks.popitem()[1]], dim=1)
        #x = self.double_conv2(x)

        #x = self.up_conv3(x)
        #x = torch.cat([x, blocks.popitem()[1]], dim=1)
        #x = self.double_conv3(x)

        #x = self.up_conv4(x)
        #x = torch.cat([x, blocks.popitem()[1]], dim=1)
        #x = self.double_conv4(x)

        #if self.concat_input:
        #    x = self.up_conv_input(x)
        #    x = torch.cat([x, input_], dim=1)
        #    x = self.double_conv_input(x)

        #x = self.final_conv(x)

        return x


def get_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b0', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b1(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b1', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b2(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b2', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b3(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b3', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b4', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b5', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b6(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b6', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model


def get_efficientunet_b7(out_channels=2, concat_input=True, pretrained=True):
    encoder = EfficientNet.encoder('efficientnet-b7', pretrained=pretrained)
    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)
    return model